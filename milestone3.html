<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">
  <link rel="icon" href="favicon.ico">

  <title>SI 612 Team 3</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
  <!-- Custom styles for this template -->
  <link href="assets/css/product.css" rel="stylesheet">
</head>
<body>

<nav id="site-header" class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
  <a id="navlogo" class="navbar-brand" href="index.html"> Team3
      <img src="assets/images/layers.svg" aria-expanded="false" width="24" height="24" alt="nav bar logo">
    </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse flex-md-row justify-content-left" id="navbarCollapse">
    <ol id="nostylelist" class="navbar-nav">
      <li class="nav-item ">
        <a class="nav-link" href="index.html">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="team.html">The Team</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="milestone1.html">Milestone 1 </a>
      </li>
      <li class="nav-item active">
        <a class="nav-link" href="milestone2.html">Milestone 2 </a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="milestone3.html">Milestone 3 </a>
      </li>
      </ul>
  </div>
</nav>


<div class="container">

  <div id="ms2" class=" col-md-auto" data-spy="scroll" data-target="#navbar">
    <div class="container section">

      <!-- the process -->
      <header class="sectiontitle">
        <h1>Milestone 3 </h1>
      </header>
      <h2>Summary</h2>
      <p>In order to learn more about the <strong>strengths</strong> and <strong>weaknesses</strong> of our initial design, we deployed a series of <strong>user enactments</strong>.
      </p>
    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Introduction</h1>
      </header>
      <p>This Milestone document covers the User Enactment study we performed to further refine our design concept.  User enactments provided our team with a powerful view of user needs and motivations via the user of props and scenarios that approximated real-life scenarios.  After conducting our enactment study, we analyzed the results, performed a new cycle of ideation, and refined our concept into the final system and demo design configurations.
      </p>
      <h2>Milestone 2 Recap</h2>
      <p>
        In Milestone 2, we developed and deployed a formative study to better understand our target audience: college students needing assistance with maintaining a healthy lifestyle through exercise.  We developed 5 overall research questions to help us gain a better understanding of this target audience, they were:
      </p>
      <ul>
        <li>RQ 1:  What motivates the Target Audience to exercise?</li>
        <li>RQ2:  What factors prevent target from exercising?</li>
        <li>RQ3:  How does Target Audience learn how to exercise?</li>
        <li>RQ4:  What kind of exercises does our Target audience actually do?</li>
        <li>RQ5:  What resources does our Target Audience use to enable their exercise?</li>
      </ul>
      <p>
        To help us better explore our research questions, we chose two specific research methodologies:
      </p>
      <ol>
        <li>A diary study where users would fill out what physical activities they did or did not do throughout the course of a week.
        </li>
        <li>An in-depth survey that which was sent out to thousands of college students to learn more about what exercises they perform, how they learned to perform them, and what resources and tools they use to help enable exercise.</li>
      </ol>

      <p>
        Each methodology was chosen specifically to help answer the research questions that we developed, with the diary study aimed at answering research questions 1 & 2, and the survey aimed at answering questions 3, 4, & 5.
      </p>
      <p>Using the two different research methods and the data we collected with each, we narrowed down our findings into three critical factors. Those factors were: time management/convenience, cardiovascular and strength training, and providing guidance on how to do exercises correctly. These factors were chosen specifically because of high response rates and consistency shown throughout both studies that each of these factors were important to our target audience.
      </p>
      <p>With these factors narrow down, we began and ideation session where we each created brand new concepts based solely around the research and the data we collected in Milestone Two. We then took all of the concepts we created (around 20) and narrowed it down to three choices which best represents the data we had collected from our research.
      </p>
      <h2>Concept 1: Life as a Gym AR Glasses</h2>
      <p>A set of smart AR glasses which we have called Life Is A Gym (LG) which are specifically geared towards the time management aspect of our research findings. When worn, these glasses integrate themselves into your everyday life and routines and help identify various exercise opportunities for the user as they go about their day. For example, if a user is walking home from class, the glasses will be able to find a route that takes them home, and while they are walking on that route, the glasses will point out various objects in the environment that the user can use to sneak in a quick workout or just help them get some more physical activity. These glasses were designed specifically to cater towards the “time management” aspect of our milestone 2 findings by helping users get in exercise without having to devote a large chunk of time of their day to go to the gym.
      </p>
      <h2>Concept 2: Smart Exercise Mat</h2>
      <p>The second concept we created was a smart yoga mat which is geared at providing guidance to our users while they perform their exercises. the smart yoga mat would feature LED panels as well as a large screen interface that can help direct users on how to perform specific exercises, for example, if a user wants to learn how to do proper push-ups a short video will appear on the screen and the exercise mat will light up specific areas for the user to place their hands and feet so they have correct form during the exercise.
      </p>
      <h2>Concept 3: Exercise Gesture guide</h2>
      <p>The final concept was a smart exercise gesture guide. This device would be a touch screen with a camera integrated into it that a user can place in front of them while performing a workout or an exercise. The guide will constantly monitor the user's movements and will be able to show the correct form based on the exercise that is being done.
      </p>
      <p>These three concepts and the research that we gathered were the main activities that were part of Milestone Two, and now Milestone 3 picks up where they left off.
      </p>
      <h2>Milestone 3 Concept Narrowing</h2>
      <p>After developing our three concepts in Milestone two, we needed to choose a specific direction to follow for the remainder of the class and our studies. We decided to look again at the data we collected for Milestone 2 and from that data it was apparent that time management was by far the largest issue that our participants addressed during our previous studies. With this in mind we decided to pursue the solution that most closely aligned with our findings and decided to work on our “Life as a Gym” Augmented Reality (AR) glasses.
      </p>
    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Study Design</h1>
      </header>
    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Study Result</h1>
      </header>
      <p>

    </div>

    <!-- The big concepts -->
    <div class="container section">
      <header class="sectiontitle">
        <h1>Ideation and Selection</h1>
      </header>
      <p>Based on the research findings, we felt like we had enough information to ideate and further refine our design concept.  To guide our refining, we used the following core principles, which were derived directly from the study findings we describe in the section above:
      </p>
      <ol>
        <li>Prioritizing individual usage</li>
        <li>User perceptions about public exercises</li>
        <li>Better user control</li>
        <li>Respect user privacy concern</li>
        <li>Supportive guidance</li>
        <li>Non-Distracting User Interface</li>
      </ol>
      <p>Below are the decisions we made regarding how LG could better serve users through recommending and fitting exercises into their daily lives.</p>
      <h3>1. Focus on individual usage scenarios</h3>
      <p>According to our research, users feel uncomfortable with involving their friends in LG activities. Most of them would not use this feature in social settings. Therefore, we decided to focus on individual usage instead of involving other users into activities. We won’t build in any activity features where multiple people are using LG routes together.
      </p>
      <h3>2. Recommending walking and running as primary exercises, climbing as secondary</h3>
      <p>We found that users feel awkward doing some exercises in public such as stretching, but walking and running are what most users are comfortable with, and climbing is what some users really enjoy. Therefore, we decided to focus on these three opportunity areas.
      </p>
      <h3>3. Display information visually and verbally, and support user input with voice and gesture control</h3>
      <p>Users worked with voice and gesture controls extensively in our study, and text is an important assistant for users especially in a noisy environment. However, the amount of text we used in the study was too much for users to read and was placed in such a manner as to impede the users sight. Therefore, we will keep the voice interaction, reduce the text to a minimum amount, and place text on side part of the LG view to ensure users can clearly see through LG glasses.
      </p>
      <h3>4. Provide multiple choices</h3>
      <p>Overall, users prefer having  moderate levels of choice, particularly in the realms of route selection and user profile preference.  When LG detects an opportunity area for users, it will provide a multiple exercise recommendations for users to choose. When users are moving from one spot to another, LG can suggest multiple routes with information such as choice in time taken, intensity of exercise, and distance.
      </p>
      <h3>5. Only track exercise relevant data</h3>
      <p>Users had privacy concerns and did not like to have LG access their friend lists, contact information and any other personal background information, so we will only track user calories and exercises for user reference.   We won’t track information such as home locations, social contacts, time spent at a location, or other details that could be used to infer personal information the user isn’t actively sharing.
      </p>
      <h3>6. Provide calories, time, and health relevant information</h3>
      <p>Based on user feedback, calories burned was the most important health information to be tracked. Time is also important because they do not want to be late due to doing extra exercises on their way, and they prefer to see how much time will take and how much time left for them to get to their destination. In addition,. Our subjects mentioned some existing products and services such as Fitbit and MyFitnessPal as comparator products that show good amounts of health information.  We will relook at these comparator products, and use their health visualizations as a template for our own.  We will also look at adding a personal goal structure to allow users to specify goals that they can measure their health information against.
      </p>
      <h3>7. Provide enough guidance for on boarding and data usage</h3>
      <p>
        LG represents powerful, emerging technology, and a correct, informative onboarding sequence seems necessary to introduce our users to the product, based on our study results. We will need to provide an onboarding walkthrough that explains basic AR concepts, as well as the primary features of the app. Our first-time setup phone experience will need to be particularly informative, so that users know what the different setting are, and how they affect the glasses experience. In addition, we will articulate how the collected data will be used, so users will be informed and will be more comfortable with providing the data.
      </p>
      <h3>8. Non-Distracting User Interface </h3>
      <p>UI design should be optimized to support the visual experience of walking/running.  UI design should be non-intrusive and minimalistic, allowing the user to maintain situational awareness of the real-world environment.
      </p>
    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>System Proposal</h1>
      </header>
      <p>
        Our final system proposal is a time-management system designed to integrate cardiovascular exercise into the daily non-exercise routine of a university student.  As the student goes about their daily exercise activity, the system will scan their calendar system and compare it with location data to generate ad-hoc walking/running routines that still fit with the students schedule.  These system consists of the following sub-components:
      </p>
      <ul>
        <li>
          A set of AR glasses that control the primary walking/running experience.  These glasses overlay the users vision, and provide Google Maps style walking/running guidance.  At the beginning of their travel routine, users put on and activate their glasses.  The AR glasses then calculate possible walking/running routes based on a variety of user-specific information, and then provides the user a choice of routes.  The primary categorization metric for the route selection will be time before the next event on the user's schedule, but the app will provide additional selection criteria, such as route distance, calories burned, points of interest along the route, and other factors.  Once a route has been selected, the glasses provide step-by-step navigation, scanning and updating the estimated time of arrival as needed based on real-world conditions.  Once the route has been completed, the glasses will provide a summary screen showing how the exercise fits into the users overall exercise experience.
        </li>
        <li>
          A companion smartphone app that lets the user view and control functionality that would be awkward to deal with while walking around.  The companion app includes the following control features:
          <ul>
            <li>The ability to enter in personal metrics (height, weight, etc) that the glasses will use to generate routes.</li>
            <li>The ability to select what calendars the glasses can see and make assumptions on.</li>
            <li>The ability to enter in personal health goals, which the glasses will also use as input into its routing algorithm.</li>
          </ul>
        </li>
      </ul>

    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1></h1>
      </header>
     </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Demo Proposal</h1>
      </header>
      <p>We will create a medium-fidelity prototype to demonstrate our unified concept to target users. We say medium-fidelity, because our concept is ultimately built around the idea that extremely light-weight, nearly-invisible AR glasses and contacts do not yet exist. Instead, we will use an existing AR glasses system to give our users the first-person perspective on what the experience would be like, assuming that those light-weight system really did exist.
      </p>
      <p>
        This demo will not be a fully-developed AR application with every feature specified in our system proposal. We have neither the time nor expertise to develop every feature listed in our system proposal. In addition, the “scanning component” of the system proposal requires image processing and context awareness technology that, so far as we are aware, does not yet exist. Our goal will be to instead create a simulation of the overall experience, using an AR app with appropriately design UI components to give the impression of a working app, and prepositioned AR markers that will resemble the experience of “scanning” the environment.
      </p>
      <p>
        <strong>Hardware:</strong> We will need to use the following hardware components to create our product:
      </p>
      <ul>
        <li>Samsung Gear VR/AR glasses (already acquired)</li>
        <li>Samsung Android-based phone compatible with Samsung Gear VR/AR (already acquired)</li>
        <li>Image Marker printouts.</li>
        <li>Earbud Audio Input/Output Solution:  We will use earbud with an in-built microphone connected to the Gear VR headset as our solution for any audio input/output needs.
        </li>
      </ul>

      <p>
          <strong>Software:</strong> We will need to use the following software components to create our product:
        </p>
      <ul>
        <li>AFrame AR/VR framework: We will build the core application in AFrame. AFrame is a web-based framework that allows for a browser-based AR experience.  An advantage of this approach is that the applications developed will run on any device that has a compatible browser (Firefox or Chromium), and access to a camera.  This gives us the ability to record experiences on both the AR glasses and with a laptop, allowing us to piece together an overall video experience using a variety of platforms.  We will run our AFrame app off a Github Pages website.
        </li>
        <li>Oculus VR App: The Gear VR requires the use of a special app that automatically activates when the phone is inserted into the Gear VR headset.  We will run our browser-based AR experience IN the Oculus VR app.
        </li>
        <li>Youtube Gaming: We would like to capture the visual experience of our AR app as prospective users experience it. Our currently plan is to use the Youtube Gaming app to capture the screen and microphone output of the Gear VR.  This approach currently suffers from one flaw:  the Gear VR app outputs in stereoscopic view, which means the recording audio is split up into two views - one for each eye.  We will continue searching for recording approaches that will better display the actual experiences users have with the system.
        </li>
      </ul>
    </div>

    <div class="container section">
        <header class="sectiontitle">
          <h1>Demo Storyboard</h1>
      </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Limitations/Shortcomings</h1>
      </header>
        <p>
          In our study, although it was exhaustive and encompassing we did encounter speed-bumps along the way. The first issue we've struggled with was a lack of time and availability to work on each phase of this project. For example, during our initial ideation phase where we were narrowing down our concept, we weren't able to consider other potential concepts that could have also solved the problem. During this step, we solidified our decision process to three ideas. Then, we narrowed down between those three with a variety of criteria. Although it wasn't documented in the best way, we could have taken a step back and spent additional time on this. In turn, it would've resulted in additional detail during the ideation phase.
        </p>
        <p>
          The next shortcoming we faced was the way our study was designed. After narrowing down to a single concept, Life As A Gym, testing the concept was next. The critical aspect of our concept was the augmented reality feature. As this technology is still relatively new, it became very clear to us that it is difficult to rapidly prototype and test in it. Therefore, we relied on using paper prototypes in our process. This was a problem because paper fails to communicate the core principle of augmented-reality. We sought to 'fake' the augmentation by holding the paper in front of the user, but regrettably, it doesn't get the same experience across.
        </p>
        <p>
          Additionally, the specific medium caused us to limit the amount of information we presented onto the paper prototypes. Because of this, we hypothesize, it had caused problems for how our users interpreted the screens and scenarios. In subsequent testing, we hope the fidelity will not be a limiting factor.
        </p>
        <p>
          With this being said, it is also important to mention the various ways our users interact with the system. Although we had hoped they would use the buttons built into the glasses, they naturally used their voice. This was a shortcoming because all of our designs were not conceptualized with the interface for button use. In retrospective, it made sense why users chose to use their voice to interact with the system. We hadn't planned for this. Thus, a contradicting statement that we've learned was that people worried about their privacy, but they naturally used their voice to interact with the system. If we had the available resources, we would have done our testing with a real augmented reality device.
        </p>
    </div>

    <div class="container section">
      <header class="sectiontitle">
        <h1>Conclusion</h1>
      </header>
      <p>
        Summing-up, for this milestone we; narrowed our focus to a single design concept, conducted and iterated through several user-enactments, had 9 unique participants and, analyzed our findings with an affinity wall. Using all of this information, we propose a plan to prototype and demonstrate a medium-fidelity prototype that incorporates our biggest findings and features. Although there were limitations with this study, we are confident in moving forward with our concept. With this being said, after completing an initial prototype, we plan to do additional tests of the prototype.
      </p>
    </div>



  </div>



  <div id="myModal" class="modal">
    <!-- The Close Button -->
    <span class="close" onclick="document.getElementById('myModal').style.display='none'">&times;</span>

    <!-- Modal Content (The Image) -->
    <img class="modal-content hidden" id="img01" src="_blank" alt="">
    <!-- Modal Caption (Image Text) -->
    <div id="caption"></div>
  </div>

  <footer class="text-muted">
    <div class="container">
      <p class="float-right">
        <a href="#">Back to top</a>
      </p>
      <p>SI 612 Project Page &copy; Team 3</p>
    </div>
  </footer>
</div>




<!-- Bootstrap core JavaScript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script>
  window.jQuery || document.write('<script src="../../assets/js/vendor/jquery-slim.min.js"><\/script>')
</script>
<script src="../../assets/js/vendor/popper.min.js"></script>
<script src="../../dist/js/bootstrap.min.js"></script>
<script src="../../assets/js/vendor/holder.min.js"></script>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script>
  Holder.addTheme('thumb', {
    bg: '#55595c',
    fg: '#eceeef',
    text: 'Thumbnail'
  });
</script>
<script>
  var modal = document.getElementById('myModal');
  modal.addEventListener('click', function() {
    this.style.display = "none";
  })

  // Get the image and insert it inside the modal - use its "alt" text as a caption
  var img = $('.myImg');
  var modalImg = $("#img01");
  var captionText = document.getElementById("caption");
  $('.myImg').click(function() {
    modal.style.display = "block";
    var newSrc = this.src;
    modalImg.attr('src', newSrc);
    modalImg.toggleClass("hidden");
    captionText.innerHTML = this.alt;
  });

  // Get the <span> element that closes the modal
  var span = document.getElementsByClassName("close")[0];

  // When the user clicks on <span> (x), close the modal
  span.onclick = function() {
    modal.style.display = "none";
  }
</script>


</body>
</html>
